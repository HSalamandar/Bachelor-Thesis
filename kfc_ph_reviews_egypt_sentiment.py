# -*- coding: utf-8 -*-
"""KFC/PH reviews Egypt sentiment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17heeKpJEV_Fo1FneigvktliG9Qgq7Mu1

INSTALLATIONS
"""

!pip install -q google_play_scraper

!pip install -q transformers #sentiment
!pip install -q plotly-express #data visualization

!pip install gensim
!pip install spacy

!python -m spacy download en_core_web_sm

!pip install pyyaml==5.4.1

"""BEGINNING OF THE CODE"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import wordcloud as wc
import string
import re
from google_play_scraper import app, Sort, reviews_all
import plotly.express as px

"""## KFC EGYPT"""

kfc_projectegy = reviews_all('com.kfc.egypt', sleep_milliseconds= 0, lang = 'en', country = 'US', sort = Sort.NEWEST)

df_egy = pd.json_normalize(kfc_projectegy)

df_egy.head(1000)

df_egy.isnull().sum()

df_egy['score'].mean()

sns.countplot(x = 'score', data = df_egy, palette = 'deep')

df_egy['reviewCreatedVersion'].value_counts()

df_egy['Sentiment'] = df_egy['score'].apply(lambda rating : 'neutral' if rating == 3 else 'positive' if rating > 3 else 'negative')

sns.countplot(x = 'Sentiment', data = df_egy)

neutral_reviews_index = df_egy[df_egy['Sentiment']=='neutral'].index

df_egy = df_egy.drop(neutral_reviews_index)

df_egy['Sentiment'].unique()

"""Sentiment Analysis Download"""

from transformers import pipeline
sentiment_analysis = pipeline("sentiment-analysis",model="siebert/sentiment-roberta-large-english")

df_egy1 = df_egy[['content', 'Sentiment']].copy()
df_egy1

df_egy1.dtypes

!pip install wordcloud
# import wordcloud as wc

text = df_egy1["content"]

wordcloud = wc.WordCloud(
    width = 3000,
    height = 2000,
    stopwords = wc.STOPWORDS).generate(str(text))

fig = plt.figure(
    figsize = (40, 30),
    facecolor = 'k',
    edgecolor = 'k')
plt.imshow(wordcloud, interpolation = 'bilinear')

"""Removing HTML tags"""

from bs4 import BeautifulSoup
!pip install lxml
df_egy1['content'] = df_egy1['content'].apply(lambda review : BeautifulSoup(review, "lxml").text)

df_egy1['content']

df_egy1['content'] = df_egy['content'].astype('str')

#df_egy['result'] = df_egy['content'].apply(lambda x : sentiment_analysis(x))

#df_egy['sentiment'] = df_egy['result'].apply(lambda x : (x[0]['label']))

df_egy.head()

# import re

df_egy1['content'] = df_egy1['content'].apply(lambda review : re.sub(r"[0-9]", " ", review.lower()))
df_egy1['content']

"""Tokenizing"""

!pip install nltk
import nltk
from nltk import TreebankWordTokenizer

tokenizer = TreebankWordTokenizer()

df_egy1['content'] = df_egy1['content'].apply(lambda review: tokenizer.tokenize(review))
df_egy1['content']

"""Remove Stopwords"""

#ALl English stopwords

!pip install --upgrade nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

stop_words = stopwords.words('english')
np.unique(stop_words)

negation_list = ["no","nor", "not", "n't"]

stop_words = [word for word in stop_words if word not in negation_list]
np.unique(stop_words)

# Remove stopwords from each review
df_egy1['content'] = df_egy1['content'].apply(lambda review: [word for word in review if word not in stop_words])

# Flatten the list of words in each review
all_words = [word for review in df_egy1['content'] for word in review]

# Get unique words
unique_words = np.unique(all_words)

print(unique_words)

"""Remove puncuations"""

# removing puncuations, and removing any dots, dashes, or empty strings

import string

df_egy1['content'] = df_egy1['content'].apply(lambda review : list(filter(None,[word for word in review if word not in string.punctuation])))

np.unique(df_egy1['content'])

#Replace multiple characters with a single space in each string
df_egy1['content'] = df_egy1['content'].apply(lambda review: list(filter(None,[re.sub(r"[-._'`*]+", '', word) for word in review])))

# Removing single Characters
df_egy1['content'] = df_egy1['content'].apply(lambda review : list(filter(None,[word for word in review if len(word)>1])))


# Print the result
np.unique(df_egy1['content'])

"""Lemmetiation/Stemming"""

nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer

lemmatizer = WordNetLemmatizer()

df_egy1['content'] = df_egy1['content'].apply(lambda review: [lemmatizer.lemmatize(word) for word in review])
np.unique(df_egy1['content'])

df_egy1.head()

"""Join lines together"""

# each review will be represented as one string

df_egy1['content']= df_egy1['content'].apply(lambda x : " ".join(x))

df_egy1['Sentiment'] = df_egy1['Sentiment'].astype('category').cat.codes

df_egy1['Sentiment']

df_egy1['content']

"""Training and Testing"""

from sklearn.model_selection import train_test_split

X = df_egy1['content'] #Independent Variable
y = df_egy1['Sentiment'] #Dependent Variable

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

good = x_train[y_train[y_train == 1].index]
bad = x_train[y_train[y_train == 0].index]

plt.figure(figsize = (20,20)) # Reviews with Good Ratings

good_reviews = wc.WordCloud(min_font_size = 3 , width = 1600 , height = 800).generate(" ".join(good))

plt.imshow(good_reviews,interpolation = 'bilinear')

plt.figure(figsize = (20,20)) # Reviews with Negative Ratings

negative_reviews = wc.WordCloud( min_font_size = 3 , width = 1600 , height = 800).generate(" ".join(bad))

plt.imshow(negative_reviews,interpolation = 'bilinear')

"""N-GRAMS COLLECTION"""

from nltk import ngrams

n_grams_good = pd.Series(ngrams(" ".join(good).split(), 3)).value_counts()[:20]
n_grams_bad = pd.Series(ngrams(" ".join(bad).split(), 3)).value_counts()[:20]

n_grams_good.sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))
plt.title('20 Most Frequently Occuring Positive N-grams')
plt.ylabel('N-gram')
plt.xlabel('No. of Occurances')

n_grams_bad.sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))
plt.title('20 Most Frequently Occuring Nergative N-grams')
plt.ylabel('N-gram')
plt.xlabel('No. of Occurances')

"""Keyword extraction"""

import collections

# Step 1: Calculate the frequency of each word in the entire dataset
word_frequency = collections.Counter([word for review in df_egy1['content'] for word in review.split()])

# Step 2: Sort the words based on their frequency
sorted_words = sorted(word_frequency.items(), key=lambda x: x[1], reverse=True)

# Step 3: Manually identify keywords related to each aspect
food_quality_keywords = {'delicious', 'tasty', 'flavorful', 'yummy', 'satisfying', 'bland', 'tasteless', 'horrible', 'amazing', 'great', 'fresh', 'quality', 'good', 'bad', 'good food', 'good quality', 'bad food', 'bad quality'}
service_keywords = {'fast', 'friendly', 'efficient', 'courteous', 'attentive', 'rude', 'slow', 'excellent', 'helpful', 'quick', 'professional', 'crash'}
ambiance_keywords = {'cozy', 'atmospheric', 'inviting', 'relaxing', 'welcoming', 'comfortable', 'clean', 'modern', 'pleasant', 'quiet', 'spacious'}
price_keywords = {'affordable', 'reasonable', 'value', 'budget-friendly', 'inexpensive', 'expensive', 'cheap', 'pricey', 'cost', 'economical', 'bad price', 'good price', 'good value', 'bad value'}

min_frequency = 1

# Step 4: Filter the most frequent words to obtain keywords related to each aspect
food_quality_keywords_freq = [(word, freq) for word, freq in sorted_words if word in food_quality_keywords and freq >= min_frequency]
service_keywords_freq = [(word, freq) for word, freq in sorted_words if word in service_keywords and freq >= min_frequency]
ambiance_keywords_freq = [(word, freq) for word, freq in sorted_words if word in ambiance_keywords and freq >= min_frequency]
price_keywords_freq = [(word, freq) for word, freq in sorted_words if word in price_keywords and freq >= min_frequency]

# Print the keywords for each aspect
print("Food Quality Keywords:")
print(food_quality_keywords_freq)
print("\nService Keywords:")
print(service_keywords_freq)
print("\nAmbiance Keywords:")
print(ambiance_keywords_freq)
print("\nPrice Keywords:")
print(price_keywords_freq)

import pandas as pd

# Assuming you have a DataFrame called df_egy containing the reviews
# And you have a list of keywords for each aspect
food_quality_keywords = ['delicious', 'tasty', 'flavorful', 'yummy', 'satisfying', 'bland', 'tasteless', 'horrible', 'amazing', 'great', 'fresh', 'quality', 'good']
service_keywords = ['fast', 'friendly', 'efficient', 'courteous', 'attentive', 'rude', 'slow', 'excellent', 'helpful', 'quick', 'professional']
ambiance_keywords = ['cozy', 'atmospheric', 'inviting', 'relaxing', 'welcoming', 'comfortable', 'clean', 'modern', 'pleasant', 'quiet', 'spacious']
price_keywords = ['affordable', 'reasonable', 'value', 'budget-friendly', 'inexpensive', 'expensive', 'cheap', 'pricey', 'cost', 'economical']

# Define a function to filter reviews based on keywords
def filter_reviews_by_keywords(df, keywords):
    filtered_reviews = df[df['content'].str.contains('|'.join(keywords), case=False)]
    return filtered_reviews

# Filter reviews for each aspect
food_quality_reviews = filter_reviews_by_keywords(df_egy, food_quality_keywords)
service_reviews = filter_reviews_by_keywords(df_egy, service_keywords)
ambiance_reviews = filter_reviews_by_keywords(df_egy, ambiance_keywords)
price_reviews = filter_reviews_by_keywords(df_egy, price_keywords)

# Print the filtered reviews
print("Food Quality Reviews:")
print(food_quality_reviews)
print("\nService Reviews:")
print(service_reviews)
print("\nAmbiance Reviews:")
print(ambiance_reviews)
print("\nPrice Reviews:")
print(price_reviews)

import pandas as pd
import matplotlib.pyplot as plt

# Positive and negative sentiment words
positive_words = {'good', 'excellent', 'delicious', 'friendly', 'efficient', 'great', 'helpful', 'quick', 'fast', 'value', 'cheap', 'clean'}
negative_words = {'bad', 'horrible', 'tasteless', 'rude', 'slow', 'terrible', 'expensive', 'value', 'crash'}

# Function to calculate sentiment distribution for each aspect
def calculate_sentiment_distribution(keywords_freq, aspect):
    positive_sentiment = sum([freq for word, freq in keywords_freq if word in positive_words])
    negative_sentiment = sum([freq for word, freq in keywords_freq if word in negative_words])
    total = positive_sentiment + negative_sentiment

    if total == 0:
        return 0, 0

    positive_percentage = positive_sentiment / total
    negative_percentage = negative_sentiment / total
    return positive_percentage, negative_percentage

# Calculate sentiment distribution for each aspect
food_quality_positive, food_quality_negative = calculate_sentiment_distribution(food_quality_keywords_freq, 'Food Quality')
service_positive, service_negative = calculate_sentiment_distribution(service_keywords_freq, 'Service')
ambiance_positive, ambiance_negative = calculate_sentiment_distribution(ambiance_keywords_freq, 'Ambiance')
price_positive, price_negative = calculate_sentiment_distribution(price_keywords_freq, 'Price')

# Create a DataFrame to store the results
sentiment_df = pd.DataFrame({
    'Aspect': ['Food Quality', 'Service', 'Ambiance', 'Price'],
    'Positive Sentiment': [food_quality_positive, service_positive, ambiance_positive, price_positive],
    'Negative Sentiment': [food_quality_negative, service_negative, ambiance_negative, price_negative]
})

# Display the DataFrame
print(sentiment_df)

# Plot sentiment distribution for all aspects
plt.figure(figsize=(10, 6))
plt.bar(sentiment_df['Aspect'], sentiment_df['Positive Sentiment'], color='blue', label='Positive Sentiment')
plt.bar(sentiment_df['Aspect'], sentiment_df['Negative Sentiment'], color='red', bottom=sentiment_df['Positive Sentiment'], label='Negative Sentiment')
plt.xlabel('Aspect')
plt.ylabel('Percentage')
plt.title('KFC Sentiment Distribution for Different Aspects in the Egypt')
plt.legend()
plt.ylim(0, 1)
plt.show()

# Calculate the total count of keywords
total_keywords_count = len(food_quality_keywords_freq) + len(service_keywords_freq) + len(ambiance_keywords_freq) + len(price_keywords_freq)

# Calculate the percentage of keywords for each aspect
food_quality_percentage = (len(food_quality_keywords_freq) / total_keywords_count) * 100
service_percentage = (len(service_keywords_freq) / total_keywords_count) * 100
ambiance_percentage = (len(ambiance_keywords_freq) / total_keywords_count) * 100
price_percentage = (len(price_keywords_freq) / total_keywords_count) * 100

# Print the percentage of keywords for each aspect
print("Food Quality Percentage:", food_quality_percentage, "%")
print("Service Percentage:", service_percentage, "%")
print("Ambiance Percentage:", ambiance_percentage, "%")
print("Price Percentage:", price_percentage, "%")

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report

X = df_egy1['content']  # Independent variable
y = df_egy1['Sentiment']  # Dependent variable

# Step 1: TF-IDF Vectorization
tfidf_vectorizer = TfidfVectorizer(max_features=5000)
X_tfidf = tfidf_vectorizer.fit_transform(X)

# Step 2: Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.3, random_state=42)

# Step 3: Train the Naive Bayes classifier
naive_bayes_classifier = MultinomialNB()
naive_bayes_classifier.fit(X_train, y_train)

# Step 4: Evaluate the model
y_pred = naive_bayes_classifier.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

from sklearn.metrics import classification_report

clf_report = pd.DataFrame(classification_report(y_test, y_pred, output_dict=True))

print("Result:")
print("===============================================")
print(f"CLASSIFICATION REPORT:\n{clf_report}")
print("_______________________________________________")
print(f"Confusion Matrix: \n {confusion_matrix(y_test, y_pred)}\n")

"""## PIZZA HUT EGYPT"""

ph_projectegy = reviews_all('com.ph.egypt', sleep_milliseconds= 0, lang = 'en', country = 'US', sort = Sort.MOST_RELEVANT)

df_egy2 = pd.json_normalize(ph_projectegy)

df_egy2.head(1000)

df_egy2.dtypes

df_egy2['score'].mean()

df_egy2['content'] = df_egy2['content'].astype('str')

#f_us['result'] = df_us['content'].apply(lambda x : sentiment_analysis(x))

sns.countplot(x = 'score', data = df_egy2, palette = 'deep')

df_egy2['Sentiment'] = df_egy2['score'].apply(lambda rating : 'neutral' if rating == 3 else 'positive' if rating > 3 else 'negative')

sns.countplot(x = 'Sentiment', data = df_egy2)

neutral_reviews_index = df_egy2[df_egy2['Sentiment']=='neutral'].index

df_egy2 = df_egy2.drop(neutral_reviews_index)

df_egy2['Sentiment'].unique()

df_egy3 = df_egy2[['content', 'Sentiment']].copy()
df_egy3

#!pip install wordcloud
# import wordcloud as wc

text = df_egy3["content"]

wordcloud = wc.WordCloud(
    width = 3000,
    height = 2000,
    stopwords = wc.STOPWORDS).generate(str(text))

fig = plt.figure(
    figsize = (40, 30),
    facecolor = 'k',
    edgecolor = 'k')
plt.imshow(wordcloud, interpolation = 'bilinear')

from bs4 import BeautifulSoup
!pip install lxml
df_egy3['content'] = df_egy3['content'].apply(lambda review : BeautifulSoup(review, "lxml").text)

df_egy3['content']

#df_us['sentiment'] = df_us['result'].apply(lambda x : (x[0]['label']))

!pip install nltk
import nltk
from nltk import TreebankWordTokenizer

tokenizer = TreebankWordTokenizer()

df_egy3['content'] = df_egy3['content'].apply(lambda review: tokenizer.tokenize(review))
df_egy3['content']

#ALl English stopwords

!pip install --upgrade nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

stop_words = stopwords.words('english')
np.unique(stop_words)

negation_list = ["no","nor", "not", "n't"]

stop_words = [word for word in stop_words if word not in negation_list]
np.unique(stop_words)

# Remove stopwords from each review
df_egy3['content'] = df_egy3['content'].apply(lambda review: [word for word in review if word not in stop_words])

# Flatten the list of words in each review
all_words = [word for review in df_egy3['content'] for word in review]

# Get unique words
unique_words = np.unique(all_words)

print(unique_words)

# removing puncuations, and removing any dots, dashes, or empty strings

import string

df_egy3['content'] = df_egy3['content'].apply(lambda review : list(filter(None,[word for word in review if word not in string.punctuation])))

np.unique(df_egy3['content'])

#Replace multiple characters with a single space in each string
df_egy3['content'] = df_egy3['content'].apply(lambda review: list(filter(None,[re.sub(r"[-._'`*]+", '', word) for word in review])))

# Removing single Characters
df_egy3['content'] = df_egy3['content'].apply(lambda review : list(filter(None,[word for word in review if len(word)>1])))


# Print the result
np.unique(df_egy3['content'])

nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer

lemmatizer = WordNetLemmatizer()

df_egy3['content'] = df_egy3['content'].apply(lambda review: [lemmatizer.lemmatize(word) for word in review])
np.unique(df_egy3['content'])

# each review will be represented as one string

df_egy3['content']= df_egy3['content'].apply(lambda x : " ".join(x))

df_egy3['content']

df_egy3['Sentiment'] = df_egy3['Sentiment'].astype('category').cat.codes

df_egy3['Sentiment']

from sklearn.model_selection import train_test_split

X = df_egy3['content'] #Independent Variable
y = df_egy3['Sentiment'] #Dependent Variable

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

good = x_train[y_train[y_train == 1].index]
bad = x_train[y_train[y_train == 0].index]

from nltk import ngrams

n_grams_good2 = pd.Series(ngrams(" ".join(good).split(), 3)).value_counts()[:20]
n_grams_bad2 = pd.Series(ngrams(" ".join(bad).split(), 3)).value_counts()[:20]

n_grams_good2.sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))
plt.title('20 Most Frequently Occuring Positive N-grams')
plt.ylabel('N-gram')
plt.xlabel('No. of Occurances')

n_grams_bad2.sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))
plt.title('20 Most Frequently Occuring Nergative N-grams')
plt.ylabel('N-gram')
plt.xlabel('No. of Occurances')

import collections

# Step 1: Calculate the frequency of each word in the entire dataset
word_frequency = collections.Counter([word for review in df_egy3['content'] for word in review.split()])

# Step 2: Sort the words based on their frequency
sorted_words = sorted(word_frequency.items(), key=lambda x: x[1], reverse=True)

# Step 3: Manually identify keywords related to each aspect
food_quality_keywords = {'food','delicious', 'tasty', 'flavorful', 'yummy', 'satisfying', 'bland', 'tasteless', 'horrible', 'amazing', 'great', 'fresh', 'quality', 'good', 'bad', 'good food', 'good quality', 'bad food', 'bad quality'}
service_keywords = {'fast', 'friendly', 'efficient', 'courteous', 'attentive', 'rude', 'slow', 'excellent', 'helpful', 'quick', 'professional', 'crash'}
ambiance_keywords = {'cozy', 'atmospheric', 'inviting', 'relaxing', 'welcoming', 'comfortable', 'clean', 'modern', 'pleasant', 'quiet', 'spacious'}
price_keywords = {'affordable', 'reasonable', 'value', 'budget-friendly', 'inexpensive', 'expensive', 'cheap', 'pricey', 'cost', 'economical', 'bad price', 'good price', 'good value', 'bad value'}

# Step 4: Define the minimum frequency threshold for each aspect
min_frequency = 1  # Adjust as needed

# Step 5: Filter the most frequent words to obtain keywords related to each aspect
food_quality_keywords_freq = [(word, freq) for word, freq in sorted_words if word in food_quality_keywords and freq >= min_frequency]
service_keywords_freq = [(word, freq) for word, freq in sorted_words if word in service_keywords and freq >= min_frequency]
ambiance_keywords_freq = [(word, freq) for word, freq in sorted_words if word in ambiance_keywords and freq >= min_frequency]
price_keywords_freq = [(word, freq) for word, freq in sorted_words if word in price_keywords and freq >= min_frequency]

# Print the keywords for each aspect
print("Food Quality Keywords:")
print(food_quality_keywords_freq)
print("\nService Keywords:")
print(service_keywords_freq)
print("\nAmbiance Keywords:")
print(ambiance_keywords_freq)
print("\nPrice Keywords:")
print(price_keywords_freq)

import pandas as pd
import matplotlib.pyplot as plt

# Positive and negative sentiment words
positive_words = {'good', 'excellent', 'delicious', 'friendly', 'efficient', 'great', 'helpful', 'quick', 'fast', 'value', 'cheap', 'clean', 'friendly', 'professional'}
negative_words = {'bad', 'horrible', 'tasteless', 'rude', 'slow', 'terrible', 'expensive'}

# Function to calculate sentiment distribution for each aspect
def calculate_sentiment_distribution(keywords_freq, aspect):
    positive_sentiment = sum([freq for word, freq in keywords_freq if word in positive_words])
    negative_sentiment = sum([freq for word, freq in keywords_freq if word in negative_words])
    total = positive_sentiment + negative_sentiment

    if total == 0:
        return 0, 0

    positive_percentage = positive_sentiment / total
    negative_percentage = negative_sentiment / total
    return positive_percentage, negative_percentage

# Calculate sentiment distribution for each aspect
food_quality_positive, food_quality_negative = calculate_sentiment_distribution(food_quality_keywords_freq, 'Food Quality')
service_positive, service_negative = calculate_sentiment_distribution(service_keywords_freq, 'Service')
ambiance_positive, ambiance_negative = calculate_sentiment_distribution(ambiance_keywords_freq, 'Ambiance')
price_positive, price_negative = calculate_sentiment_distribution(price_keywords_freq, 'Price')

# Create a DataFrame to store the results
sentiment_df1 = pd.DataFrame({
    'Aspect': ['Food Quality', 'Service', 'Ambiance', 'Price'],
    'Positive Sentiment': [food_quality_positive, service_positive, ambiance_positive, price_positive],
    'Negative Sentiment': [food_quality_negative, service_negative, ambiance_negative, price_negative]
})

# Display the DataFrame
print(sentiment_df1)

# Plot sentiment distribution for all aspects
plt.figure(figsize=(10, 6))
plt.bar(sentiment_df1['Aspect'], sentiment_df1['Positive Sentiment'], color='blue', label='Positive Sentiment')
plt.bar(sentiment_df1['Aspect'], sentiment_df1['Negative Sentiment'], color='red', bottom=sentiment_df1['Positive Sentiment'], label='Negative Sentiment')
plt.xlabel('Aspect')
plt.ylabel('Percentage')
plt.title('Pizza Hut Sentiment Distribution for Different Aspects in the Egypt')
plt.legend()
plt.ylim(0, 1)
plt.show()

# Calculate the total count of keywords
total_keywords_count = len(food_quality_keywords_freq) + len(service_keywords_freq) + len(ambiance_keywords_freq) + len(price_keywords_freq)

# Calculate the percentage of keywords for each aspect
food_quality_percentage = (len(food_quality_keywords_freq) / total_keywords_count) * 100
service_percentage = (len(service_keywords_freq) / total_keywords_count) * 100
ambiance_percentage = (len(ambiance_keywords_freq) / total_keywords_count) * 100
price_percentage = (len(price_keywords_freq) / total_keywords_count) * 100

# Print the percentage of keywords for each aspect
print("Food Quality Percentage:", food_quality_percentage, "%")
print("Service Percentage:", service_percentage, "%")
print("Ambiance Percentage:", ambiance_percentage, "%")
print("Price Percentage:", price_percentage, "%")

df_egy2.head(1000)

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report

# Step 1: Prepare your data (assuming df_egy has already been preprocessed)
X = df_egy3['content']  # Independent variable
y = df_egy3['Sentiment']  # Dependent variable

# Step 2: TF-IDF Vectorization
tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # You can adjust max_features
X_tfidf = tfidf_vectorizer.fit_transform(X)

# Step 3: Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.3, random_state=42)

# Step 4: Train the Naive Bayes classifier
naive_bayes_classifier = MultinomialNB()
naive_bayes_classifier.fit(X_train, y_train)

# Step 5: Evaluate the model
y_pred = naive_bayes_classifier.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# Classification report
print("\nClassification Report:")
print(classification_report(y_test, y_pred))